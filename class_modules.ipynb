{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import scipy.optimize as spop\n",
    "from itertools import product\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "from binance.client import AsyncClient, HistoricalKlinesType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: orange\">Binance_Batch_Klines_Downloader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = await AsyncClient.create()\n",
    "\n",
    "class Binance_Batch_Klines_Downloader:\n",
    "  def __init__(self, days_back = 2, interval = \"1h\", klines_types=[HistoricalKlinesType.FUTURES], rate_limit_ps=2):\n",
    "    self.days_back = days_back\n",
    "    self.start = str(datetime.utcnow() - timedelta(days = self.days_back))\n",
    "    \n",
    "    self.interval = interval\n",
    "    self.klines_types = klines_types\n",
    "    self.rate_limit_ps = rate_limit_ps\n",
    "    \n",
    "  def _bars_to_df(self, bars):\n",
    "    df = pd.DataFrame(bars)\n",
    "    df[\"Date\"] = pd.to_datetime(df.iloc[:,0], unit = \"ms\")\n",
    "    df.columns = [\"Open Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "                  \"Close Time\", \"Quote Asset Volume\", \"Number of Trades\",\n",
    "                  \"Taker Buy Base Asset Volume\", \"Taker Buy Quote Asset Volume\", \"Ignore\", \"Date\"]\n",
    "    df = df[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
    "    df.set_index(\"Date\", inplace = True)\n",
    "    for column in df.columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors = \"coerce\")\n",
    "        \n",
    "    return df\n",
    "  \n",
    "  async def _get_symbols(self, klines_type):\n",
    "    exchange_info = None\n",
    "    if klines_type == HistoricalKlinesType.FUTURES:\n",
    "      exchange_info = await client.futures_exchange_info()\n",
    "    if klines_type == HistoricalKlinesType.SPOT:\n",
    "      exchange_info = await client.get_exchange_info()\n",
    "    \n",
    "    if exchange_info is not None:\n",
    "      return list(map(lambda x: x['symbol'], exchange_info['symbols']))\n",
    "  \n",
    "  async def _download_kline_type(self, klines_type):\n",
    "    symbols = await self._get_symbols(klines_type)\n",
    "    if not len(symbols):\n",
    "      return\n",
    "    \n",
    "    new_dir = \"Binance_Historical_%s_%s_%i_days_%s\" % (self.interval, klines_type.name, self.days_back, str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    os.mkdir(new_dir)\n",
    "    \n",
    "    chunks = []\n",
    "    for idx, _ in enumerate(symbols):\n",
    "      if idx % self.rate_limit_ps == 0:\n",
    "        symbols_chunk = symbols[idx:idx+self.rate_limit_ps]\n",
    "        futures_chunk = []\n",
    "        \n",
    "        for symbol in symbols_chunk:\n",
    "          futures_chunk.append(client.get_historical_klines(symbol = symbol, interval = self.interval,\n",
    "                                        start_str = self.start, end_str = None, limit = 1000, klines_type=klines_type))\n",
    "          \n",
    "        chunks.append(futures_chunk)\n",
    "        \n",
    "    for index, chunk in enumerate(chunks):\n",
    "      print(\"Fetching data for ↓ %s %s\" % (klines_type.name, self.interval), \"%i / %i\" % (index, len(chunks)))\n",
    "      results = await asyncio.gather(*chunk)\n",
    "      \n",
    "      for i, bars in enumerate(results):\n",
    "        print(symbols[index*self.rate_limit_ps+i])\n",
    "        try:\n",
    "          df = self._bars_to_df(bars)\n",
    "          df.to_csv(\"%s/%s_%s_%s.csv\" % (new_dir, symbols[index*self.rate_limit_ps+i], klines_type.name, self.interval))\n",
    "        except:\n",
    "          # print(\"raw: \", bars)\n",
    "          print(\"Couldn't construct DataFrame from raw data for %s\" % symbols[index*self.rate_limit_ps+i])\n",
    "          \n",
    "  async def download(self):\n",
    "    for type in self.klines_types:\n",
    "      print(type)\n",
    "      await self._download_kline_type(type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: orange\">DEMO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = Binance_Batch_Klines_Downloader(days_back=20, interval=\"1h\", klines_types=[HistoricalKlinesType.FUTURES], rate_limit_ps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistoricalKlinesType.FUTURES\n",
      "Fetching data for ↓ FUTURES 1h 0 / 26\n",
      "BTCUSDT\n",
      "ETHUSDT\n",
      "BCHUSDT\n",
      "XRPUSDT\n",
      "EOSUSDT\n",
      "LTCUSDT\n",
      "TRXUSDT\n",
      "Fetching data for ↓ FUTURES 1h 1 / 26\n",
      "ETCUSDT\n",
      "LINKUSDT\n",
      "XLMUSDT\n",
      "ADAUSDT\n",
      "XMRUSDT\n",
      "DASHUSDT\n",
      "ZECUSDT\n",
      "Fetching data for ↓ FUTURES 1h 2 / 26\n",
      "XTZUSDT\n",
      "BNBUSDT\n",
      "ATOMUSDT\n",
      "ONTUSDT\n",
      "IOTAUSDT\n",
      "BATUSDT\n",
      "VETUSDT\n",
      "Fetching data for ↓ FUTURES 1h 3 / 26\n",
      "NEOUSDT\n",
      "QTUMUSDT\n",
      "IOSTUSDT\n",
      "THETAUSDT\n",
      "ALGOUSDT\n",
      "ZILUSDT\n",
      "KNCUSDT\n",
      "Fetching data for ↓ FUTURES 1h 4 / 26\n",
      "ZRXUSDT\n",
      "COMPUSDT\n",
      "OMGUSDT\n",
      "DOGEUSDT\n",
      "SXPUSDT\n",
      "KAVAUSDT\n",
      "BANDUSDT\n",
      "Fetching data for ↓ FUTURES 1h 5 / 26\n",
      "RLCUSDT\n",
      "WAVESUSDT\n",
      "MKRUSDT\n",
      "SNXUSDT\n",
      "DOTUSDT\n",
      "DEFIUSDT\n",
      "YFIUSDT\n",
      "Fetching data for ↓ FUTURES 1h 6 / 26\n",
      "BALUSDT\n",
      "CRVUSDT\n",
      "TRBUSDT\n",
      "RUNEUSDT\n",
      "SUSHIUSDT\n",
      "SRMUSDT\n",
      "EGLDUSDT\n",
      "Fetching data for ↓ FUTURES 1h 7 / 26\n",
      "SOLUSDT\n",
      "ICXUSDT\n",
      "STORJUSDT\n",
      "BLZUSDT\n",
      "UNIUSDT\n",
      "AVAXUSDT\n",
      "FTMUSDT\n",
      "Fetching data for ↓ FUTURES 1h 8 / 26\n",
      "HNTUSDT\n",
      "ENJUSDT\n",
      "FLMUSDT\n",
      "TOMOUSDT\n",
      "RENUSDT\n",
      "KSMUSDT\n",
      "NEARUSDT\n",
      "Fetching data for ↓ FUTURES 1h 9 / 26\n",
      "AAVEUSDT\n",
      "FILUSDT\n",
      "RSRUSDT\n",
      "LRCUSDT\n",
      "MATICUSDT\n",
      "OCEANUSDT\n",
      "CVCUSDT\n",
      "Fetching data for ↓ FUTURES 1h 10 / 26\n",
      "BELUSDT\n",
      "CTKUSDT\n",
      "AXSUSDT\n",
      "ALPHAUSDT\n",
      "ZENUSDT\n",
      "SKLUSDT\n",
      "GRTUSDT\n",
      "Fetching data for ↓ FUTURES 1h 11 / 26\n",
      "1INCHUSDT\n",
      "BTCBUSD\n",
      "CHZUSDT\n",
      "SANDUSDT\n",
      "ANKRUSDT\n",
      "BTSUSDT\n",
      "LITUSDT\n",
      "Fetching data for ↓ FUTURES 1h 12 / 26\n",
      "UNFIUSDT\n",
      "REEFUSDT\n",
      "RVNUSDT\n",
      "SFPUSDT\n",
      "XEMUSDT\n",
      "BTCSTUSDT\n",
      "COTIUSDT\n",
      "Fetching data for ↓ FUTURES 1h 13 / 26\n",
      "CHRUSDT\n",
      "MANAUSDT\n",
      "ALICEUSDT\n",
      "HBARUSDT\n",
      "ONEUSDT\n",
      "LINAUSDT\n",
      "STMXUSDT\n",
      "Fetching data for ↓ FUTURES 1h 14 / 26\n",
      "DENTUSDT\n",
      "CELRUSDT\n",
      "HOTUSDT\n",
      "MTLUSDT\n",
      "OGNUSDT\n",
      "NKNUSDT\n",
      "SCUSDT\n",
      "Fetching data for ↓ FUTURES 1h 15 / 26\n",
      "DGBUSDT\n",
      "1000SHIBUSDT\n",
      "ICPUSDT\n",
      "BAKEUSDT\n",
      "GTCUSDT\n",
      "ETHBUSD\n",
      "BTCDOMUSDT\n",
      "Fetching data for ↓ FUTURES 1h 16 / 26\n",
      "TLMUSDT\n",
      "BNBBUSD\n",
      "ADABUSD\n",
      "XRPBUSD\n",
      "IOTXUSDT\n",
      "DOGEBUSD\n",
      "AUDIOUSDT\n",
      "Fetching data for ↓ FUTURES 1h 17 / 26\n",
      "RAYUSDT\n",
      "C98USDT\n",
      "MASKUSDT\n",
      "ATAUSDT\n",
      "SOLBUSD\n",
      "FTTBUSD\n",
      "DYDXUSDT\n",
      "Fetching data for ↓ FUTURES 1h 18 / 26\n",
      "1000XECUSDT\n",
      "GALAUSDT\n",
      "CELOUSDT\n",
      "ARUSDT\n",
      "KLAYUSDT\n",
      "ARPAUSDT\n",
      "CTSIUSDT\n",
      "Fetching data for ↓ FUTURES 1h 19 / 26\n",
      "LPTUSDT\n",
      "ENSUSDT\n",
      "PEOPLEUSDT\n",
      "ANTUSDT\n",
      "ROSEUSDT\n",
      "DUSKUSDT\n",
      "FLOWUSDT\n",
      "Fetching data for ↓ FUTURES 1h 20 / 26\n",
      "IMXUSDT\n",
      "API3USDT\n",
      "GMTUSDT\n",
      "APEUSDT\n",
      "BNXUSDT\n",
      "WOOUSDT\n",
      "FTTUSDT\n",
      "Fetching data for ↓ FUTURES 1h 21 / 26\n",
      "JASMYUSDT\n",
      "DARUSDT\n",
      "GALUSDT\n",
      "AVAXBUSD\n",
      "NEARBUSD\n",
      "GMTBUSD\n",
      "APEBUSD\n",
      "Fetching data for ↓ FUTURES 1h 22 / 26\n",
      "GALBUSD\n",
      "FTMBUSD\n",
      "DODOBUSD\n",
      "ANCBUSD\n",
      "GALABUSD\n",
      "TRXBUSD\n",
      "1000LUNCBUSD\n",
      "Fetching data for ↓ FUTURES 1h 23 / 26\n",
      "LUNA2BUSD\n",
      "OPUSDT\n",
      "DOTBUSD\n",
      "TLMBUSD\n",
      "ICPBUSD\n",
      "BTCUSDT_220930\n",
      "ETHUSDT_220930\n",
      "Fetching data for ↓ FUTURES 1h 24 / 26\n",
      "WAVESBUSD\n",
      "LINKBUSD\n",
      "SANDBUSD\n",
      "LTCBUSD\n",
      "MATICBUSD\n",
      "CVXBUSD\n",
      "FILBUSD\n",
      "Fetching data for ↓ FUTURES 1h 25 / 26\n",
      "1000SHIBBUSD\n",
      "LEVERBUSD\n"
     ]
    }
   ],
   "source": [
    "await downloader.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:purple;\">Kalman Filters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KalmanFilterAverage(x):\n",
    "  # Construct a Kalman filter\n",
    "    kf = KalmanFilter(transition_matrices = [1],\n",
    "    observation_matrices = [1],\n",
    "    initial_state_mean = 0,\n",
    "    initial_state_covariance = 1,\n",
    "    observation_covariance=1,\n",
    "    transition_covariance=.01)\n",
    "  # Use the observed values of the price to get a rolling mean\n",
    "    state_means, _ = kf.filter(x.values)\n",
    "    state_means = pd.Series(state_means.flatten(), index=x.index)\n",
    "    return state_means\n",
    "# Kalman filter regression\n",
    "def KalmanFilterRegression(x,y):\n",
    "    delta = 1e-3\n",
    "    trans_cov = delta / (1 - delta) * np.eye(2) # How much random walk wiggles\n",
    "    obs_mat = np.expand_dims(np.vstack([[x], [np.ones(len(x))]]).T, axis=1)\n",
    "    kf = KalmanFilter(n_dim_obs=1, n_dim_state=2, # y is 1-dimensional, (alpha, beta) is 2-dimensional\n",
    "    initial_state_mean=[0,0],\n",
    "    initial_state_covariance=np.ones((2, 2)),\n",
    "    transition_matrices=np.eye(2),\n",
    "    observation_matrices=obs_mat,\n",
    "    observation_covariance=2,\n",
    "    transition_covariance=trans_cov)\n",
    "    # Use the observations y to get running estimates and errors for the state parameters\n",
    "    state_means, state_covs = kf.filter(y.values)\n",
    "    return state_means\n",
    "def half_life(spread):\n",
    "    spread_lag = spread.shift(1)\n",
    "    spread_lag.iloc[0] = spread_lag.iloc[1]\n",
    "    spread_ret = spread - spread_lag\n",
    "    spread_ret.iloc[0] = spread_ret.iloc[1]\n",
    "    spread_lag2 = sm.add_constant(spread_lag)\n",
    "    model = sm.OLS(spread_ret,spread_lag2)\n",
    "    res = model.fit()\n",
    "    halflife = int(round(-np.log(2) / res.params[1],0))\n",
    "    if halflife <= 0:\n",
    "        halflife = 1\n",
    "    return halflife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:lime;\">Coint_Analyzer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Digger:\n",
    "  def __init__(self, dir_paths=[\"Binance_Historical_1h_FUTURES_20_days_2022-07-22T17:05:35\"], observations_low_pass = 0):\n",
    "    self.dir_paths = dir_paths\n",
    "    self.observations_low_pass = observations_low_pass\n",
    "    \n",
    "    self.df = None\n",
    "    self.corr_pairs = None\n",
    "    self.coint_pairs = None\n",
    "    self.corr_coint_pairs = None\n",
    "    \n",
    "  def process_raw_data(self, dir_paths=None):\n",
    "    if dir_paths is not None:\n",
    "      self.dir_paths = dir_paths\n",
    "    self._closings_csv_to_df()\n",
    "    self._raw_to_processed()\n",
    "    self.get_trading_pairs()\n",
    "  \n",
    "  def _closings_csv_to_df(self):\n",
    "    # reading Close values and merging to one DF\n",
    "    df_closings = pd.DataFrame()\n",
    "    \n",
    "    for path in self.dir_paths:\n",
    "      with os.scandir('raw_data/%s' % path) as entries:\n",
    "          for entry in entries:\n",
    "            instrument = \"_\".join(entry.name.split(\"_\")[0:2])\n",
    "            df = pd.read_csv('raw_data/%s/%s' % (path, entry.name), index_col=\"Date\")\n",
    "            df = df[[\"Close\"]].copy()\n",
    "            df.columns = [instrument]\n",
    "            df_closings = pd.concat([df_closings, df], axis=1)\n",
    "    \n",
    "    # filtering data based on amount of observations in DF\n",
    "    df_observation_num = pd.DataFrame(columns=[\"observations\"])\n",
    "    for column in df_closings.columns:\n",
    "      df_observation_num.loc[column] = len(df_closings[column].dropna())\n",
    "\n",
    "    drop_columns = []\n",
    "    for _, row in df_observation_num.iterrows():\n",
    "      if row.observations < self.observations_low_pass: # arbitrarily selected value based on bottom values from df_observation_num\n",
    "        drop_columns.append(row.name)\n",
    "        \n",
    "    # removing outliers from the original DF\n",
    "    df_closings.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "    # cleaning DF\n",
    "    df_closings.dropna(inplace=True)\n",
    "            \n",
    "    self.df = df_closings\n",
    "    \n",
    "  def _raw_to_processed(self):\n",
    "    # CORRELATION\n",
    "    if self.df is None:\n",
    "      return\n",
    "    \n",
    "    matrix = self.df.pct_change().corr(method ='pearson')\n",
    "    matrix.to_excel(\"processed_data/corr_matrix_temp_%s.xlsx\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    \n",
    "    au_corr = matrix.corr().unstack()\n",
    "    labels_to_drop = self._get_redundant_corr_pairs(matrix)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    au_corr.dropna(inplace=True)\n",
    "    \n",
    "    indexes = []\n",
    "    values = []\n",
    "    for idx in au_corr.index:\n",
    "        indexes.append(\"%s-%s\" % (idx[0], idx[1]))\n",
    "        values.append(au_corr[idx])\n",
    "    corr_pairs_df = pd.DataFrame(index=indexes, data=values)\n",
    "    \n",
    "    self.corr_pairs = corr_pairs_df\n",
    "    try:\n",
    "        corr_pairs_df.to_csv(\"processed_data/corr_pairs__tem%s.csv\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    except:\n",
    "      print(\"Couldn't save pairs to temp files\")\n",
    "      \n",
    "    # COINTEGRATION \n",
    "    _, coint_pairs = self._find_cointegrated_pairs(self.df)\n",
    "    self.coint_pairs = coint_pairs\n",
    "    \n",
    "    \n",
    "  def _get_redundant_corr_pairs(self, df_corr_matrix):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df_corr_matrix.columns\n",
    "    for i in range(0, df_corr_matrix.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "    \n",
    "  def _find_cointegrated_pairs(self, df):\n",
    "    n = df.shape[1]\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = df.keys()\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            print(\"Performing coint test %s %s %s\" % (j, i, n))\n",
    "            \n",
    "            result = coint(df[keys[i]], df[keys[j]])\n",
    "            pvalue_matrix[i, j] = result[1]\n",
    "            \n",
    "            # testing for spread stationarity\n",
    "            if result[1] < 0.05:\n",
    "              state_means = KalmanFilterRegression(KalmanFilterAverage(df[keys[i]]),KalmanFilterAverage(df[keys[j]]))\n",
    "              hedge_ratio = - state_means[:,0]\n",
    "              spread = df[keys[j]] + (df[keys[i]] * hedge_ratio)\n",
    "              result_adf = adfuller(spread)\n",
    "              \n",
    "              if result_adf[1] < 0.01 and result_adf[0] < result_adf[4][\"1%\"]:\n",
    "                # is mean reverting\n",
    "                hurst = self._get_hurst_exponent(np.array(spread))\n",
    "                if hurst <= 0.5:\n",
    "                  pairs.append((keys[i], keys[j], result[1], result_adf[0], hurst))\n",
    "    try:\n",
    "        indexes = []\n",
    "        adf = []\n",
    "        hurst = []\n",
    "        for row in pairs:\n",
    "            indexes.append(\"%s-%s\" % (row[0], row[1]))\n",
    "            adf.append(row[3])\n",
    "            hurst.append(row[4])\n",
    "            \n",
    "        coint_pairs_df = pd.DataFrame(index=indexes)\n",
    "        coint_pairs_df['adf'] = adf\n",
    "        coint_pairs_df['hurst'] = hurst\n",
    "        coint_pairs_df.sort_values(ascending=True, by=\"adf\")\n",
    "        coint_pairs_df.to_csv(\"processed_data/coint_pairs_temp%s.csv\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "        \n",
    "        pv_val_df = pd.DataFrame(pvalue_matrix)\n",
    "        pv_val_df.columns = df.columns\n",
    "        pv_val_df.index = df.columns\n",
    "        pv_val_df.to_excel(\"processed_data/coint_matrix_temp_%s.xlsx\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    except:\n",
    "       print(\"Couldn't save pairs to temp files\") \n",
    "    return pvalue_matrix, coint_pairs_df\n",
    "  \n",
    "  def _get_hurst_exponent(self, time_series):\n",
    "    \"\"\"Returns the Hurst Exponent of the time series vector ts\"\"\"\n",
    "    # Create the range of lag values\n",
    "    lags = range(2, 20)\n",
    "    # Calculate the array of the variances of the lagged differences\n",
    "    tau = [np.sqrt(np.std(np.subtract(time_series[lag:], time_series[:-lag]))) for lag in lags]\n",
    "    # Use a linear fit to estimate the Hurst Exponent\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    # Return the Hurst exponent from the polyfit output\n",
    "    return poly[0]*2.0\n",
    "  \n",
    "  \n",
    "    \n",
    "  def get_trading_pairs(self, h_pass = 0.95, corr_path = None, coint_path = None):\n",
    "    df_corr = None\n",
    "    df_coint = None\n",
    "    if corr_path is not None and coint_path is not None:\n",
    "      df_corr = pd.read_csv(corr_path)\n",
    "      df_coint = pd.read_csv(coint_path)\n",
    "    elif self.corr_pairs is not None and self.coint_pairs is not None:\n",
    "      df_corr = self.corr_pairs.copy()\n",
    "      df_coint = self.coint_pairs.copy()\n",
    "      \n",
    "    if df_corr is None or df_coint is None:\n",
    "      return\n",
    "    \n",
    "    df_hi_corr = df_corr.loc[df_corr[0]>h_pass]\n",
    "    df_corr_coint_pairs = pd.DataFrame(columns=[\"corr\", \"adf\", \"hurst\"])\n",
    "    for idx in df_hi_corr.index:\n",
    "      if idx in df_coint.index:\n",
    "        df_corr_coint_pairs.loc[idx] = [df_hi_corr.loc[idx][0], df_coint.loc[idx][0], df_coint.loc[idx][1]]\n",
    "        \n",
    "    self.corr_coint_pairs = df_corr_coint_pairs\n",
    "    try:\n",
    "      df_corr_coint_pairs.to_csv(\"processed_data/corr_coint_pairs_temp_%s.csv\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    except:\n",
    "      print(\"Data couldn't be stored in a static file.\")\n",
    "    return df_corr_coint_pairs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:lime;\">DEMO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Data_Digger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.process_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adf</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FLOWUSDT_FUTURES-HBARUSDT_FUTURES</th>\n",
       "      <td>-4.891916</td>\n",
       "      <td>0.379599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOWUSDT_FUTURES-DGBUSDT_FUTURES</th>\n",
       "      <td>-10.983471</td>\n",
       "      <td>0.414408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOWUSDT_FUTURES-GRTUSDT_FUTURES</th>\n",
       "      <td>-10.334058</td>\n",
       "      <td>0.445274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOWUSDT_FUTURES-1000XECUSDT_FUTURES</th>\n",
       "      <td>-11.665109</td>\n",
       "      <td>0.382401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOWUSDT_FUTURES-TRXUSDT_FUTURES</th>\n",
       "      <td>-11.472901</td>\n",
       "      <td>0.270961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALICEUSDT_FUTURES-DOGEBUSD_FUTURES</th>\n",
       "      <td>-6.007009</td>\n",
       "      <td>0.363962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALICEUSDT_FUTURES-MANAUSDT_FUTURES</th>\n",
       "      <td>-21.094305</td>\n",
       "      <td>0.320501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENTUSDT_FUTURES-TLMUSDT_FUTURES</th>\n",
       "      <td>-5.048487</td>\n",
       "      <td>0.475382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICPBUSD_FUTURES-DOGEBUSD_FUTURES</th>\n",
       "      <td>-4.765577</td>\n",
       "      <td>0.166342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QTUMUSDT_FUTURES-KSMUSDT_FUTURES</th>\n",
       "      <td>-4.129591</td>\n",
       "      <td>0.336203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            adf     hurst\n",
       "FLOWUSDT_FUTURES-HBARUSDT_FUTURES     -4.891916  0.379599\n",
       "FLOWUSDT_FUTURES-DGBUSDT_FUTURES     -10.983471  0.414408\n",
       "FLOWUSDT_FUTURES-GRTUSDT_FUTURES     -10.334058  0.445274\n",
       "FLOWUSDT_FUTURES-1000XECUSDT_FUTURES -11.665109  0.382401\n",
       "FLOWUSDT_FUTURES-TRXUSDT_FUTURES     -11.472901  0.270961\n",
       "...                                         ...       ...\n",
       "ALICEUSDT_FUTURES-DOGEBUSD_FUTURES    -6.007009  0.363962\n",
       "ALICEUSDT_FUTURES-MANAUSDT_FUTURES   -21.094305  0.320501\n",
       "DENTUSDT_FUTURES-TLMUSDT_FUTURES      -5.048487  0.475382\n",
       "ICPBUSD_FUTURES-DOGEBUSD_FUTURES      -4.765577  0.166342\n",
       "QTUMUSDT_FUTURES-KSMUSDT_FUTURES      -4.129591  0.336203\n",
       "\n",
       "[434 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.coint_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corr</th>\n",
       "      <th>adf</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ETHUSDT_FUTURES-ETHBUSD_FUTURES</th>\n",
       "      <td>0.999968</td>\n",
       "      <td>-5.599590</td>\n",
       "      <td>0.053953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLBUSD_FUTURES-SOLUSDT_FUTURES</th>\n",
       "      <td>0.999965</td>\n",
       "      <td>-19.659554</td>\n",
       "      <td>0.329978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTCUSDT_FUTURES-BTCBUSD_FUTURES</th>\n",
       "      <td>0.999955</td>\n",
       "      <td>-4.911036</td>\n",
       "      <td>0.119993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEARUSDT_FUTURES-NEARBUSD_FUTURES</th>\n",
       "      <td>0.999928</td>\n",
       "      <td>-19.203775</td>\n",
       "      <td>0.370388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVAXBUSD_FUTURES-AVAXUSDT_FUTURES</th>\n",
       "      <td>0.999905</td>\n",
       "      <td>-6.441516</td>\n",
       "      <td>0.345058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNIUSDT_FUTURES-TRBUSDT_FUTURES</th>\n",
       "      <td>0.950540</td>\n",
       "      <td>-9.175631</td>\n",
       "      <td>0.394841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTKUSDT_FUTURES-XRPBUSD_FUTURES</th>\n",
       "      <td>0.950321</td>\n",
       "      <td>-13.327771</td>\n",
       "      <td>0.404883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FILUSDT_FUTURES-ICPBUSD_FUTURES</th>\n",
       "      <td>0.950211</td>\n",
       "      <td>-9.881951</td>\n",
       "      <td>0.446731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTCUSDT_FUTURES-1000SHIBUSDT_FUTURES</th>\n",
       "      <td>0.950131</td>\n",
       "      <td>-6.078477</td>\n",
       "      <td>0.396348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AXSUSDT_FUTURES-OGNUSDT_FUTURES</th>\n",
       "      <td>0.950096</td>\n",
       "      <td>-6.290114</td>\n",
       "      <td>0.133364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          corr        adf     hurst\n",
       "ETHUSDT_FUTURES-ETHBUSD_FUTURES       0.999968  -5.599590  0.053953\n",
       "SOLBUSD_FUTURES-SOLUSDT_FUTURES       0.999965 -19.659554  0.329978\n",
       "BTCUSDT_FUTURES-BTCBUSD_FUTURES       0.999955  -4.911036  0.119993\n",
       "NEARUSDT_FUTURES-NEARBUSD_FUTURES     0.999928 -19.203775  0.370388\n",
       "AVAXBUSD_FUTURES-AVAXUSDT_FUTURES     0.999905  -6.441516  0.345058\n",
       "...                                        ...        ...       ...\n",
       "UNIUSDT_FUTURES-TRBUSDT_FUTURES       0.950540  -9.175631  0.394841\n",
       "CTKUSDT_FUTURES-XRPBUSD_FUTURES       0.950321 -13.327771  0.404883\n",
       "FILUSDT_FUTURES-ICPBUSD_FUTURES       0.950211  -9.881951  0.446731\n",
       "GTCUSDT_FUTURES-1000SHIBUSDT_FUTURES  0.950131  -6.078477  0.396348\n",
       "AXSUSDT_FUTURES-OGNUSDT_FUTURES       0.950096  -6.290114  0.133364\n",
       "\n",
       "[197 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.get_trading_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:salmon\">Backtester</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backtester:\n",
    "  def __init__(self, dir_name, data_interval, pairs, tc):\n",
    "    self.tc = tc\n",
    "    self.dir_name = dir_name\n",
    "    self.data_interval = data_interval\n",
    "    \n",
    "    self.pair_results = None\n",
    "    self.pair_results_opt_overview = None\n",
    "    \n",
    "    self.pairs = pairs\n",
    "    self.pairs_results = None\n",
    "    self.pairs_results_opt_overview = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.data = None\n",
    "    self.tp_year = None\n",
    "    \n",
    "    self._get_data()\n",
    "    \n",
    "    \n",
    "  def __repr__(self):\n",
    "      return \"Pairs trading backtester\"\n",
    "        \n",
    "  def _get_data(self):\n",
    "    entries = []\n",
    "    for pair in self.pairs:\n",
    "      for instrument in pair:\n",
    "        entries.append(\"%s_%s.csv\" % (instrument, self.data_interval))\n",
    "    # removing duplicates\n",
    "    entries = list(set(entries))\n",
    "      \n",
    "    df_closings = pd.DataFrame()\n",
    "    for entry in entries:\n",
    "      instrument = \"_\".join(entry.split(\"_\")[0:-1])\n",
    "      df = pd.read_csv('raw_data/%s/%s' % (self.dir_name, entry), index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "      df = df[[\"Close\"]].copy()\n",
    "      df.columns = [instrument]\n",
    "      df[\"%s_returns\" % instrument] = np.log(df[instrument]/df[instrument].shift(1))\n",
    "      df_closings = pd.concat([df_closings, df], axis=1)\n",
    "      \n",
    "      \n",
    "    df_closings.dropna(inplace=True)\n",
    "    df_closings = df_closings.T.drop_duplicates().T\n",
    "    self.data = df_closings\n",
    "    self.tp_year = (self.data.shape[0] / ((self.data.index[-1] - self.data.index[0]).days / 365.25))\n",
    "    \n",
    "    return df_closings\n",
    "  \n",
    "  def backtest_pairs(self, optimum_metrics=\"SHARPE\"):\n",
    "    # run self.optimize_pairs\n",
    "    # read optimization data\n",
    "    # run backtest with the optimal\n",
    "    \n",
    "    return\n",
    "  \n",
    "  def optimize_pairs(self, z_entry_range=(1.8, 3.2), z_exit_range=(-3, 0)):\n",
    "    pairs_results_opt_overview = pd.DataFrame(columns=[\"opt_entry\", \"opt_exit\", \"net\", \"sharpe\"])\n",
    "    for idx, pair in enumerate(self.pairs):\n",
    "      print(\"OPTIMIZING \", idx, \"/\", len(self.pairs), \": \", \"-\".join(pair))\n",
    "      self.optimize_pair(z_entry_range=z_entry_range, z_exit_range=z_exit_range, pair=pair, metrics=\"SHARPE\")\n",
    "      top_params = self.pair_results_opt_overview.iloc[0]\n",
    "      pairs_results_opt_overview.loc[\"-\".join(pair)] = [top_params.z_entry, top_params.z_exit, top_params.net, top_params.sharpe]\n",
    "      \n",
    "    pairs_results_opt_overview.sort_values(by=\"net\", ascending=False, inplace=True)\n",
    "    self.pairs_results_opt_overview = pairs_results_opt_overview\n",
    "    return pairs_results_opt_overview\n",
    "    \n",
    "  \n",
    "  # does stop-losses/take-profit make sense?\n",
    "  def backtest_pair(self, pair=None, z_entry=2, z_exit=0, rolling_z_score=False):\n",
    "    if pair is None: \n",
    "      return\n",
    "    \n",
    "    inst1 = pair[0]\n",
    "    inst2 = pair[1]\n",
    "    df = self.data.copy()\n",
    "    \n",
    "    state_means = KalmanFilterRegression(KalmanFilterAverage(df[inst1]), KalmanFilterAverage(df[inst2]))\n",
    "    df['hr'] = - state_means[:, 0]\n",
    "    df['spread'] = df[inst2] + (df[inst1]*df.hr)\n",
    "    \n",
    "    \n",
    "    halflife = 0\n",
    "    if rolling_z_score is True:\n",
    "      halflife = half_life(df.spread)\n",
    "    \n",
    "    if halflife > 1:\n",
    "        spread_mean = df.spread.rolling(center=False, window=halflife).mean()\n",
    "        spread_std = df.spread.rolling(center=False, window=halflife).std()\n",
    "        spread_rolling = df.spread.rolling(center=False, window=1).mean()\n",
    "        df['z_score'] = (df.spread-spread_mean)/spread_std\n",
    "    else:\n",
    "        df['z_score'] = (df.spread-df.spread.mean())/df.spread.std()\n",
    "    \n",
    "    # set up num_units_long\n",
    "    df['long_entry'] = ((df.z_score < - z_entry) & ( df.z_score.shift(1) > - z_entry))\n",
    "    df['long_exit'] = ((df.z_score > - z_exit) & (df.z_score.shift(1) < - z_exit))\n",
    "    df['num_units_long'] = np.nan \n",
    "    df.loc[df['long_entry'],'num_units_long'] = 1 \n",
    "    df.loc[df['long_exit'],'num_units_long'] = 0 \n",
    "    df['num_units_long'][0] = 0 \n",
    "    df['num_units_long'] = df['num_units_long'].fillna(method='pad') \n",
    "    # set up num units short \n",
    "    df['short_entry'] = ((df.z_score > z_entry) & ( df.z_score.shift(1) < z_entry))\n",
    "    df['short_exit'] = ((df.z_score < z_exit) & (df.z_score.shift(1) > z_exit))\n",
    "    df.loc[df['short_entry'],'num_units_short'] = -1\n",
    "    df.loc[df['short_exit'],'num_units_short'] = 0\n",
    "    df['num_units_short'][0] = 0\n",
    "    df['num_units_short'] = df['num_units_short'].fillna(method='pad')\n",
    "    df['num_units'] = df['num_units_long'] + df['num_units_short']\n",
    "    # exit at the end of testing period\n",
    "    df['num_units'][-1] = 0\n",
    "    \n",
    "    df['dailyret'] = (df['%s_returns' % inst1]* -df['num_units'].shift(1)) + (df['%s_returns' % inst2]*df['num_units'].shift(1))\n",
    "    df['gross'] = df['dailyret'].cumsum()\n",
    "\n",
    "    df['trades'] = df.num_units.diff().fillna(0).abs()*2\n",
    "    dailyret_net = df['dailyret'] - df['trades']*self.tc\n",
    "    df['net'] = dailyret_net.cumsum()\n",
    "    \n",
    "    # CAGR calculation - not in use for now\n",
    "    # start_val = 1\n",
    "    # end_val = df['gross'].iat[-1]\n",
    "    # start_date = df.iloc[0].name\n",
    "    # end_date = df.iloc[-1].name\n",
    "    # days = (end_date - start_date).days\n",
    "    # CAGR = round(((float(end_val) / float(start_val)) ** (self.tp_year/days)) - 1,4)\n",
    "    \n",
    "    self.pair_results = df\n",
    "    return df.net\n",
    "  \n",
    "  def optimize_pair(self, z_entry_range, z_exit_range, step=0.1, pair=None, save_to_file=False, metrics=\"SHARPE\", rolling_z_score=False):\n",
    "    z_entry_range = np.arange(*z_entry_range, step)\n",
    "    z_entry_range = np.round(z_entry_range, 2)\n",
    "    z_exit_range = np.arange(*z_exit_range, step)\n",
    "    z_exit_range = np.round(z_exit_range, 2)\n",
    "    combinations = list(product(z_entry_range, z_exit_range))\n",
    "    \n",
    "    net_returns = []\n",
    "    sharpe_ratios = []\n",
    "    max_drawdowns = []\n",
    "    for idx, comb in enumerate(combinations):\n",
    "      self.backtest_pair(pair, z_entry=comb[0], z_exit=comb[1], rolling_z_score=rolling_z_score)\n",
    "      ##############################################################\n",
    "      try:\n",
    "          sharpe = ((self.pair_results['net'].mean() / self.pair_results['net'].std()) * sqrt(self.tp_year))\n",
    "      except ZeroDivisionError:\n",
    "          sharpe = 0.0\n",
    "      ##############################################################\n",
    "      max_dd, _, _ = self.calculate_max_dd(self.pair_results['net'])\n",
    "      \n",
    "      net_returns.append(self.pair_results[\"net\"][-1])\n",
    "      sharpe_ratios.append(sharpe)\n",
    "      max_drawdowns.append(max_dd)\n",
    "      \n",
    "      # tracking the comb checking progress\n",
    "      print(len(combinations), idx, comb, net_returns[-1], sharpe_ratios[-1], max_dd)\n",
    "      \n",
    "    pair_results_opt_overview = pd.DataFrame(data = np.array(combinations), columns = [\"z_entry\", \"z_exit\"])\n",
    "    pair_results_opt_overview[\"net\"] = net_returns\n",
    "    pair_results_opt_overview[\"sharpe\"] = sharpe_ratios\n",
    "    pair_results_opt_overview[\"max_dd\"] = max_drawdowns\n",
    "    \n",
    "    if metrics == \"SHARPE\":\n",
    "      pair_results_opt_overview.sort_values(by=\"sharpe\", ascending=False, inplace=True)\n",
    "    elif metrics == \"RETURN\":\n",
    "      pair_results_opt_overview.sort_values(by=\"net\", ascending=False, inplace=True)\n",
    "    \n",
    "    self.pair_results_opt_overview = pair_results_opt_overview\n",
    "    \n",
    "    return pair_results_opt_overview\n",
    "  \n",
    "  def calculate_max_dd(self, cumret):\n",
    "  # ======================================================\n",
    "  # calculation of maximum drawdown and maximum drawdown duration based on\n",
    "  # cumulative COMPOUNDED returns. cumret must be a compounded cumulative return.\n",
    "  # i is the index of the day with maxDD.\n",
    "  # ======================================================\n",
    "    highwatermark=np.zeros(cumret.shape)\n",
    "    drawdown=np.zeros(cumret.shape)\n",
    "    drawdown_duration=np.zeros(cumret.shape)\n",
    "    for t in np.arange(1, cumret.shape[0]):\n",
    "        highwatermark[t]=np.maximum(highwatermark[t-1],\n",
    "        cumret[t])\n",
    "        drawdown[t]=(1+cumret[t])/(1+highwatermark[t])-1\n",
    "        if drawdown[t]==0:\n",
    "            drawdown_duration[t]=0\n",
    "        else:\n",
    "            drawdown_duration[t]=drawdown_duration[t-1]+1\n",
    "    max_dd, i = np.min(drawdown), np.argmin(drawdown) \n",
    "    # drawdown < 0 always\n",
    "    max_ddd=np.max(drawdown_duration)\n",
    "    return max_dd, max_ddd, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:salmon\">DEMO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "str_pairs = pd.read_csv(\"./processed_data/corr_coint_pairs_temp_2022-07-31T16:59:28.csv\").iloc[:,0].tolist()\n",
    "for str_pair in str_pairs:\n",
    "  pairs.append(str_pair.split(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "backtester = Backtester(pairs=pairs, dir_name=\"Binance_Historical_15m_FUTURES_20_days_2022-07-14T12:00:43\", data_interval=\"15m\", tc=0.0006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pairs trading backtester(INSTRUMENTS = FLOWUSDT_FUTURES / TRXUSDT_FUTURES)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtester.optimize_pairs(z_entry_range=(1.3, 2.2), z_exit_range=(-1.2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>EXAMPLES + PLOTTING IDEAS</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester.backtest_pair(pair=backtester.pair, z_entry=2.6, z_exit=-0.1, rolling_z_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester.pair_results['net'].plot(figsize=(12, 8))\n",
    "backtester.pair_results['gross'].plot(figsize=(12, 8))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester.optimize_pair(z_entry_range=(2, 2.6), z_exit_range=(-0.6, 0), pair=backtester.pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_opt_overview = backtester.pair_results_opt_overview.copy()\n",
    "norm_opt_overview['sharpe'] = norm_opt_overview['sharpe']/norm_opt_overview['sharpe'].shift()\n",
    "norm_opt_overview['net'] = norm_opt_overview['net']/norm_opt_overview['net'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester.pair_results_opt_overview.groupby(\"z_entry\").sharpe.mean().plot()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "backtester.pair_results_opt_overview.groupby(\"z_exit\").sharpe.mean().plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester.pair_results_opt_overview.groupby(\"z_entry\").net.mean().plot()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "backtester.pair_results_opt_overview.groupby(\"z_exit\").net.mean().plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for optimization I will probably need more simplified (df based) ver of the strategy\n",
    "# # advanced tester below should be finished and revised\n",
    "# def test_strategy_advanced(self, window=24, spread_entry=0.04, spread_exit=0.001, tp=0, sl=-0.04, critical_threshold = -2.4, unbiased_adf=True):\n",
    "# inst1 = self.pair[0]\n",
    "# inst2 = self.pair[1]\n",
    "\n",
    "# data = self.data.copy()\n",
    "# data[\"signals\"] =  0\n",
    "# data[\"%s_position\" % inst1] = 0\n",
    "# data[\"%s_position\" % inst2] = 0\n",
    "# data[\"gross_returns\"] = 0\n",
    "# data[\"net_returns\"] = 0\n",
    "\n",
    "# signal = 0\n",
    "# old_signal = 0\n",
    "# current_return = 0\n",
    "# position0 = 0\n",
    "# position1 = 0\n",
    "\n",
    "# #moving through the sample\n",
    "# for t in range(window, len(data)-1):\n",
    "# df_adfuller = pd.DataFrame()\n",
    "\n",
    "# # because of sampling index is off when sample[t]\n",
    "# sample = data.iloc[t-window:t+1].copy()\n",
    "\n",
    "# old_signal = signal\n",
    "# old_position0 = position0\n",
    "# old_position1 = position1\n",
    "# gross = 0\n",
    "# net = 0\n",
    "\n",
    "# if unbiased_adf is True:\n",
    "# def unit_root(b):\n",
    "#     a = np.average(sample[inst2] - b*sample[inst1])\n",
    "#     df_adfuller[\"fair_value\"] = a + b*sample[inst1]\n",
    "#     df_adfuller[\"diff_fair\"] = df_adfuller[\"fair_value\"] - sample[inst2]\n",
    "#     df_adfuller[\"delta_diff_fair\"] = df_adfuller[\"diff_fair\"]/df_adfuller[\"diff_fair\"].shift(1)\n",
    "#     df_adfuller[\"lag_diff_fair\"] = df_adfuller[\"diff_fair\"].shift()\n",
    "    \n",
    "#     # LAGS (augmentation)\n",
    "#     # df_adfuller[\"lag1_delta_diff_fair\"] = df_adfuller[\"delta_diff_fair\"].shift(1)\n",
    "#     # df_adfuller[\"lag2_delta_diff_fair\"] = df_adfuller[\"delta_diff_fair\"].shift(2)\n",
    "    \n",
    "#     df_adfuller.dropna(inplace=True)\n",
    "#     #OLS(dependent, independent)\n",
    "#     reg = sm.OLS(df_adfuller[\"delta_diff_fair\"], df_adfuller[[\"lag_diff_fair\"]]) #, \"lag1_delta_diff_fair\", \"lag2_delta_diff_fair\"\n",
    "#     res = reg.fit()\n",
    "#     return res.params[0]/res.bse[0]\n",
    "  \n",
    "# # starting point\n",
    "# reg = sm.OLS(np.array(sample[inst2]), sm.add_constant(sample[inst1]))\n",
    "# res = reg.fit()\n",
    "# b0 = res.params[1]\n",
    "\n",
    "# # optimising the cointegration equation parameters - method most probably can be more performant one\n",
    "# res1 = spop.minimize(unit_root, b0, method='Nelder-Mead')\n",
    "# t_opt = res1.fun\n",
    "# b_opt = float(res1.x)\n",
    "# a_opt = np.average(sample[inst2] - b_opt*sample[inst1])\n",
    "\n",
    "# # Z-Score based entries\n",
    "# # sample[\"fair_value\"] = a_opt + b_opt*sample[inst1]\n",
    "# # sample[\"fair_spread\"] = sample[\"fair_value\"] - sample[inst2]\n",
    "# # spread_mean = np.mean(sample[\"fair_spread\"])\n",
    "# # spread_std = np.std(sample[\"fair_spread\"])\n",
    "# # # investigate the curvature of the z_score in comparison to spread\n",
    "# # sample[\"z_score\"] = (sample[\"fair_spread\"]-spread_mean)/spread_std\n",
    "# else:\n",
    "# print(\"BIASED ADFULLER\")\n",
    "\n",
    "# #simulating trading\n",
    "# fair_value = a_opt + b_opt*sample[inst1].iloc[window]\n",
    "# spread = fair_value - sample[inst2].iloc[window]\n",
    "\n",
    "# # print(t, fair_value, spread, t_opt, signal)\n",
    "\n",
    "# if t_opt > critical_threshold:\n",
    "# signal = 0\n",
    "# gross = 0\n",
    "# else: \n",
    "# if old_signal == 0:\n",
    "#   # pattern below is used in order to achieve result in % regarding the entry value\n",
    "#   # abs(raw_data[tickers[1]][t]/(a_opt + b_opt*raw_data[tickers[0]][t])-1) < entry \n",
    "#   # spread in % ~ \n",
    "#   if abs(spread) > spread_entry: \n",
    "#     signal = np.sign(sample[inst2].iloc[window] - fair_value)\n",
    "# elif old_signal != 0:\n",
    "#   if sl != 0 and current_return < sl:\n",
    "#     signal = 0\n",
    "#   elif tp != 0 and current_return > tp:\n",
    "#     signal = 0\n",
    "#   elif abs(spread) <= spread_exit:\n",
    "#     signal = 0\n",
    "#   elif np.sign(fair_value - sample[inst2].iloc[window]) != old_signal:\n",
    "#     if abs(spread) >= spread_entry:\n",
    "#       signal = np.sign(sample[inst2].iloc[window] - fair_value)\n",
    "#     else:\n",
    "#       signal = 0\n",
    "#   # below should be the equivalent (~) of\n",
    "#   # elif np.sign(raw_data[tickers[1]][t] - (a_opt + b_opt*raw_data[tickers[0]][t])) == old_signal:\n",
    "#   #   singal = old_signal\n",
    "#   elif abs(spread) > spread_entry:\n",
    "#     signal = np.sign(sample[inst2].iloc[window] - fair_value)\n",
    "  \n",
    "# position0 = signal\n",
    "# position1 = -signal\n",
    "\n",
    "# # gross = position0*(sample[inst1][t+1]/sample[inst1][t] - 1) + position1*(sample[inst2[1]][t+1]/sample[inst2[1]][t] - 1)\n",
    "# gross = position0*sample[\"%s_returns\" % inst1][window] + position1*sample[\"%s_returns\" % inst2][window]\n",
    "# net = gross - self.tc*(abs(position0 - old_position0) + abs(position1 - old_position1))\n",
    "# if signal == old_signal:\n",
    "#   current_return = (1+current_return)*(1+gross)-1\n",
    "# else:\n",
    "#   current_return = gross\n",
    "\n",
    "# # data is not assigned correctly or sth\n",
    "\n",
    "# # adfuller data could also be assigned below\n",
    "# data.loc[[data.index[t]], [\"%s_position\" % inst1]] = position0\n",
    "# data.loc[[data.index[t]], [\"%s_position\" % inst2]] = position1\n",
    "# data.loc[[data.index[t]], [\"gross_returns\"] ] = gross\n",
    "# data.loc[[data.index[t]], [\"net_returns\"]] = net\n",
    "\n",
    "# #interface: reporting daily positions and realised returns\n",
    "# print('day '+str(data.index[t]), t, \"/\", len(data)-1)\n",
    "# print('')\n",
    "# if signal == 0:\n",
    "#     print('no trading')\n",
    "# elif  signal == 1:\n",
    "#     print('long position on '+inst2+' and short position on '+inst1)\n",
    "# else:\n",
    "#     print('long position on '+inst1+' and short position on '+inst2)\n",
    "# print('gross daily return: '+str(round(gross*100,2))+'%')\n",
    "# print('net daily return: '+str(round(net*100,2))+'%')\n",
    "# print('cumulative net return so far: '+str(round(np.prod(1+data[\"net_returns\"])*100-100,2))+'%')\n",
    "# print('')\n",
    "\n",
    "# results = data\n",
    "\n",
    "# #plotting equity curves\n",
    "# # plt.plot(np.append(1,np.cumprod(1+data[\"gross_returns\"])))\n",
    "# # plt.plot(np.append(1,np.cumprod(1+data[\"net_returns\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('trader_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51043f12cc41a0415ec9a5864812a206c32759eeedd2c2d6292bc5f1056404b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
