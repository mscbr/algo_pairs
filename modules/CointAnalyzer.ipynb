{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests, coint\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:purple;\">Kalman Filters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster Kalman Filter from IterativeBacktester can be implemented\n",
    "def KalmanFilterAverage(x):\n",
    "  # Construct a Kalman filter\n",
    "    kf = KalmanFilter(transition_matrices = [1],\n",
    "    observation_matrices = [1],\n",
    "    initial_state_mean = 0,\n",
    "    initial_state_covariance = 1,\n",
    "    observation_covariance=1,\n",
    "    transition_covariance=.01)\n",
    "  # Use the observed values of the price to get a rolling mean\n",
    "    state_means, _ = kf.filter(x.values)\n",
    "    state_means = pd.Series(state_means.flatten(), index=x.index)\n",
    "    return state_means\n",
    "# Kalman filter regression\n",
    "def KalmanFilterRegression(x,y):\n",
    "    delta = 1e-3\n",
    "    trans_cov = delta / (1 - delta) * np.eye(2) # How much random walk wiggles\n",
    "    obs_mat = np.expand_dims(np.vstack([[x], [np.ones(len(x))]]).T, axis=1)\n",
    "    kf = KalmanFilter(n_dim_obs=1, n_dim_state=2, # y is 1-dimensional, (alpha, beta) is 2-dimensional\n",
    "    initial_state_mean=[0,0],\n",
    "    initial_state_covariance=np.ones((2, 2)),\n",
    "    transition_matrices=np.eye(2),\n",
    "    observation_matrices=obs_mat,\n",
    "    observation_covariance=2,\n",
    "    transition_covariance=trans_cov)\n",
    "    # Use the observations y to get running estimates and errors for the state parameters\n",
    "    state_means, state_covs = kf.filter(y.values)\n",
    "    return state_means\n",
    "def half_life(spread):\n",
    "    spread_lag = spread.shift(1)\n",
    "    spread_lag.iloc[0] = spread_lag.iloc[1]\n",
    "    spread_ret = spread - spread_lag\n",
    "    spread_ret.iloc[0] = spread_ret.iloc[1]\n",
    "    spread_lag2 = sm.add_constant(spread_lag)\n",
    "    model = sm.OLS(spread_ret,spread_lag2)\n",
    "    res = model.fit()\n",
    "    halflife = int(round(-np.log(2) / res.params[1],0))\n",
    "    if halflife <= 1:\n",
    "        halflife = 1\n",
    "    return halflife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:lime;\">Coint_Analyzer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coint_Analyzer:\n",
    "  # raw_data_path, save_dir_path\n",
    "  def __init__(self, dir_paths=[\"Binance_Historical_15m_FUTURES_20_days_2022-07-14T12:00:43\"], observations_low_pass = 0):\n",
    "    self.dir_paths = dir_paths\n",
    "    self.observations_low_pass = observations_low_pass\n",
    "    \n",
    "    self.df = None\n",
    "    self.corr_pairs = None\n",
    "    self.coint_pairs = None\n",
    "    self.corr_coint_pairs = None\n",
    "    \n",
    "    self._closings_csv_to_df()\n",
    "    \n",
    "  def process_raw_data(self, dir_paths=None):\n",
    "    if dir_paths is not None:\n",
    "      self.dir_paths = dir_paths\n",
    "    \n",
    "    self._raw_to_processed()\n",
    "    self.get_trading_pairs()\n",
    "  \n",
    "  def _closings_csv_to_df(self):\n",
    "    # reading Close values and merging to one DF\n",
    "    df_closings = pd.DataFrame()\n",
    "    \n",
    "    for path in self.dir_paths:\n",
    "      with os.scandir('../raw_data/%s' % path) as entries:\n",
    "          for entry in entries:\n",
    "            instrument = \"_\".join(entry.name.split(\"_\")[0:2])\n",
    "            df = pd.read_csv('../raw_data/%s/%s' % (path, entry.name), index_col=\"Date\")\n",
    "            df = df[[\"Close\"]].copy()\n",
    "            df.columns = [instrument]\n",
    "            df_closings = pd.concat([df_closings, df], axis=1)\n",
    "    \n",
    "    # filtering data based on amount of observations in DF\n",
    "    df_observation_num = pd.DataFrame(columns=[\"observations\"])\n",
    "    for column in df_closings.columns:\n",
    "      df_observation_num.loc[column] = len(df_closings[column].dropna())\n",
    "\n",
    "    drop_columns = []\n",
    "    for _, row in df_observation_num.iterrows():\n",
    "      if row.observations < self.observations_low_pass: # arbitrarily selected value based on bottom values from df_observation_num\n",
    "        drop_columns.append(row.name)\n",
    "        \n",
    "    # removing outliers from the original DF\n",
    "    df_closings.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "    # cleaning DF\n",
    "    df_closings.dropna(inplace=True)\n",
    "            \n",
    "    self.df = df_closings\n",
    "    \n",
    "  def _raw_to_processed(self):\n",
    "    # CORRELATION\n",
    "    if self.df is None:\n",
    "      return\n",
    "    \n",
    "    matrix = self.df.pct_change().corr(method ='pearson')\n",
    "    matrix.to_excel(\"processed_data/corr_matrix_temp_%s.xlsx\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    \n",
    "    au_corr = matrix.corr().unstack()\n",
    "    labels_to_drop = self._get_redundant_corr_pairs(matrix)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    au_corr.dropna(inplace=True)\n",
    "    \n",
    "    indexes = []\n",
    "    values = []\n",
    "    for idx in au_corr.index:\n",
    "        indexes.append(\"%s-%s\" % (idx[0], idx[1]))\n",
    "        values.append(au_corr[idx])\n",
    "    corr_pairs_df = pd.DataFrame(index=indexes, data=values)\n",
    "    \n",
    "    self.corr_pairs = corr_pairs_df\n",
    "    try:\n",
    "        corr_pairs_df.to_csv(\"../processed_data/corr_pairs__tem%s.csv\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    except:\n",
    "      print(\"Couldn't save pairs to temp files\")\n",
    "      \n",
    "    # COINTEGRATION \n",
    "    _, coint_pairs = self._find_cointegrated_pairs(self.df.copy())\n",
    "    self.coint_pairs = coint_pairs\n",
    "    \n",
    "    \n",
    "  def _get_redundant_corr_pairs(self, df_corr_matrix):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df_corr_matrix.columns\n",
    "    for i in range(0, df_corr_matrix.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "    \n",
    "  def _find_cointegrated_pairs(self, df):\n",
    "    n = df.shape[1]\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = df.copy().keys()\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            print(\"Performing coint test %s %s %s\" % (j, i, n))\n",
    "            \n",
    "            result = coint(df[keys[i]], df[keys[j]])\n",
    "            pvalue_matrix[i, j] = result[1]\n",
    "            \n",
    "            # testing for spread stationarity\n",
    "            if result[1] < 0.05:\n",
    "              state_means = KalmanFilterRegression(KalmanFilterAverage(df[keys[i]]), KalmanFilterAverage(df[keys[j]]))\n",
    "              hedge_ratio = - state_means[:,0]\n",
    "              spread = df[keys[j]] + (df[keys[i]] * hedge_ratio)\n",
    "              result_adf = adfuller(spread)\n",
    "              \n",
    "              if result_adf[1] < 0.01 and result_adf[0] < result_adf[4][\"1%\"]:\n",
    "                # Granger causality test\n",
    "                # maxlag value should be investigated\n",
    "                # why halflife f() didn't want to work :thinking_face:\n",
    "                try:\n",
    "                  g12_pval = grangercausalitytests(df[[keys[i], keys[j]]], maxlag=1, verbose=False)[1][0]['ssr_chi2test'][1]\n",
    "                  g21_pval = grangercausalitytests(df[[keys[j], keys[i]]], maxlag=1, verbose=False)[1][0]['ssr_chi2test'][1]\n",
    "                except:\n",
    "                  g12_pval = 0\n",
    "                  g21_pval = 0\n",
    "                  \n",
    "                # is it mean reverting\n",
    "                hurst = self._get_hurst_exponent(np.array(spread))\n",
    "                if hurst <= 0.5:\n",
    "                  pairs.append((keys[i], keys[j], result[1], result_adf[0], hurst, g12_pval, g21_pval))\n",
    "    try:\n",
    "        indexes = []\n",
    "        adf = []\n",
    "        hurst = []\n",
    "        granger_12 = []\n",
    "        granger_21 = []\n",
    "        for row in pairs:\n",
    "            indexes.append(\"%s-%s\" % (row[0], row[1]))\n",
    "            adf.append(row[3])\n",
    "            hurst.append(row[4])\n",
    "            granger_12.append(row[5])\n",
    "            granger_21.append(row[6])\n",
    "            \n",
    "        coint_pairs_df = pd.DataFrame(index=indexes)\n",
    "        coint_pairs_df['adf'] = adf\n",
    "        coint_pairs_df['hurst'] = hurst\n",
    "        coint_pairs_df['granger_12'] = granger_12\n",
    "        coint_pairs_df['granger_21'] = granger_21\n",
    "        coint_pairs_df.sort_values(ascending=True, by=\"adf\")\n",
    "        coint_pairs_df.to_csv(\"../processed_data/coint_pairs_temp%s.csv\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "        \n",
    "        pv_val_df = pd.DataFrame(pvalue_matrix)\n",
    "        pv_val_df.columns = df.columns\n",
    "        pv_val_df.index = df.columns\n",
    "        pv_val_df.to_excel(\"../processed_data/coint_matrix_temp_%s.xlsx\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    except:\n",
    "       print(\"Couldn't save pairs to temp files\") \n",
    "    return pvalue_matrix, coint_pairs_df\n",
    "  \n",
    "  def _get_hurst_exponent(self, time_series):\n",
    "    \"\"\"Returns the Hurst Exponent of the time series vector ts\"\"\"\n",
    "    # Create the range of lag values\n",
    "    lags = range(2, 20)\n",
    "    # Calculate the array of the variances of the lagged differences\n",
    "    tau = [np.sqrt(np.std(np.subtract(time_series[lag:], time_series[:-lag]))) for lag in lags]\n",
    "    # Use a linear fit to estimate the Hurst Exponent\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    # Return the Hurst exponent from the polyfit output\n",
    "    return poly[0]*2.0\n",
    "    \n",
    "  def get_trading_pairs(self, h_pass = 0.95, corr_path = None, coint_path = None):\n",
    "    df_corr = None\n",
    "    df_coint = None\n",
    "    if corr_path is not None and coint_path is not None:\n",
    "      df_corr = pd.read_csv(corr_path)\n",
    "      df_coint = pd.read_csv(coint_path)\n",
    "    elif self.corr_pairs is not None and self.coint_pairs is not None:\n",
    "      df_corr = self.corr_pairs.copy()\n",
    "      df_coint = self.coint_pairs.copy()\n",
    "      \n",
    "    if df_corr is None or df_coint is None:\n",
    "      return\n",
    "    \n",
    "    df_hi_corr = df_corr.loc[df_corr[0]>h_pass]\n",
    "    df_corr_coint_pairs = pd.DataFrame(columns=[\"corr\", \"adf\", \"hurst\", \"granger_12\", \"granger_21\"])\n",
    "    for idx in df_hi_corr.index:\n",
    "      if idx in df_coint.index:\n",
    "        df_corr_coint_pairs.loc[idx] = [df_hi_corr.loc[idx][0], df_coint.loc[idx][0], df_coint.loc[idx][1], df_coint.loc[idx][2], df_coint.loc[idx][3]]\n",
    "        \n",
    "    self.corr_coint_pairs = df_corr_coint_pairs\n",
    "    try:\n",
    "      df_corr_coint_pairs.to_csv(\"../processed_data/corr_coint_pairs_temp_%s.csv\" % str(datetime.utcnow().replace(microsecond=0).isoformat()))\n",
    "    except:\n",
    "      print(\"Data couldn't be stored in a static file.\")\n",
    "    return df_corr_coint_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:lime;\">DEMO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Coint_Analyzer(observations_low_pass=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.process_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.get_trading_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.corr_coint_pairs.loc[(analyzer.corr_coint_pairs.granger_12 <= 0.05) | (analyzer.corr_coint_pairs.granger_21 <= 0.05)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('trader_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51043f12cc41a0415ec9a5864812a206c32759eeedd2c2d6292bc5f1056404b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
